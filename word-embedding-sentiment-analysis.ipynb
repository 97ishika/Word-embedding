{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "df = pd.read_csv('/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv', encoding='utf-8')\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['sentiment'] == 'positive', 'sentiment'] = 1\n",
    "df.loc[df['sentiment'] == 'negative', 'sentiment'] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...         1\n",
       "1  A wonderful little production. <br /><br />The...         1\n",
       "2  I thought this was a wonderful way to spend ti...         1\n",
       "3  Basically there's a family where a little boy ...         0\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...         1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "review_lines = list()\n",
    "lines = df['review'].values.tolist()\n",
    "\n",
    "for line in lines:   \n",
    "    tokens = word_tokenize(line)\n",
    "    # convert to lower case\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    # remove punctuation from each word    \n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    # filter out stop words    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    review_lines.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(review_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 134156\n"
     ]
    }
   ],
   "source": [
    "import gensim \n",
    "\n",
    "EMBEDDING_DIM = 100\n",
    "# train word2vec model\n",
    "model = gensim.models.Word2Vec(sentences=review_lines, size=EMBEDDING_DIM, window=5, workers=4, min_count=1)\n",
    "# vocab size\n",
    "words = list(model.wv.vocab)\n",
    "print('Vocabulary size: %d' % len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model in ASCII (word2vec) format\n",
    "filename = 'imdb_embedding_word2vec.txt'\n",
    "model.wv.save_word2vec_format(filename, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('terrible', 0.9369164705276489),\n",
       " ('awful', 0.8719069957733154),\n",
       " ('horrid', 0.8024217486381531),\n",
       " ('atrocious', 0.7962092161178589),\n",
       " ('horrendous', 0.7941433191299438),\n",
       " ('pathetic', 0.7894285917282104),\n",
       " ('dreadful', 0.7854096293449402),\n",
       " ('lousy', 0.7846511006355286),\n",
       " ('sucks', 0.782234787940979),\n",
       " ('laughable', 0.742453932762146)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let us try some utility functions of gensim word2vec more details here \n",
    "\n",
    "model.wv.most_similar('horrible')#, topn =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('juliet', 0.8808293342590332),\n",
       " ('romeo', 0.8786265850067139),\n",
       " ('princess', 0.850227952003479),\n",
       " ('tromeo', 0.8299891352653503),\n",
       " ('lion', 0.8298365473747253),\n",
       " ('bride', 0.8290273547172546),\n",
       " ('lennie', 0.8252159953117371),\n",
       " ('prince', 0.8228528499603271),\n",
       " ('queen', 0.8216102719306946),\n",
       " ('cordaraby', 0.8215664029121399)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let’s see the result of semantically reasonable word vectors (king - man + woman)\n",
    "model.wv.most_similar_cosmul(positive=['woman', 'king'], negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lion', 0.6205703616142273),\n",
       " ('princess', 0.5915178656578064),\n",
       " ('juliet', 0.5899780988693237),\n",
       " ('romeo', 0.58808434009552),\n",
       " ('bride', 0.5648108124732971),\n",
       " ('rice', 0.5589447021484375),\n",
       " ('hawkings', 0.5551849603652954),\n",
       " ('lear', 0.555041491985321),\n",
       " ('queen', 0.5493825674057007),\n",
       " ('cordaraby', 0.539921760559082)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Let’s see the result of semantically reasonable word vectors (king - man + woman)\n",
    "model.wv.most_similar(positive=['woman', 'king'], negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movie\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/gensim/models/keyedvectors.py:877: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
     ]
    }
   ],
   "source": [
    "#odd word out\n",
    "print(model.wv.doesnt_match(\"woman king queen movie\".split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dog', 0.7618103623390198),\n",
       " ('hat', 0.7261778116226196),\n",
       " ('mouse', 0.7034851908683777),\n",
       " ('soup', 0.6988158226013184),\n",
       " ('chicken', 0.6888816952705383),\n",
       " ('snake', 0.6759913563728333),\n",
       " ('rabbit', 0.6749992370605469),\n",
       " ('dude', 0.6657153367996216),\n",
       " ('monkey', 0.6647992134094238),\n",
       " ('bugs', 0.6627668142318726)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similar_by_word(\"cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8303565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "print(model.similarity('boy', 'girl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join('', 'imdb_embedding_word2vec.txt'),  encoding = \"utf-8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:])\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df.loc[:24999, 'review'].values\n",
    "y_train = df.loc[:24999, 'sentiment'].values\n",
    "X_test = df.loc[25000:, 'review'].values\n",
    "y_test = df.loc[25000:, 'sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_reviews = X_train + X_test\n",
    "max_length = 100 # try other options like mean of sentence lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 134156 unique tokens.\n",
      "Shape of review tensor: (50000, 100)\n",
      "Shape of sentiment tensor: (50000,)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "# vectorize the text samples into a 2D integer tensor\n",
    "tokenizer_obj = Tokenizer()\n",
    "tokenizer_obj.fit_on_texts(review_lines)\n",
    "sequences = tokenizer_obj.texts_to_sequences(review_lines)\n",
    "\n",
    "# pad sequences\n",
    "word_index = tokenizer_obj.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "review_pad = pad_sequences(sequences, maxlen=max_length)\n",
    "sentiment =  df['sentiment'].values\n",
    "print('Shape of review tensor:', review_pad.shape)\n",
    "print('Shape of sentiment tensor:', sentiment.shape)\n",
    "\n",
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(review_pad.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "review_pad = review_pad[indices]\n",
    "sentiment = sentiment[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * review_pad.shape[0])\n",
    "\n",
    "X_train_pad = review_pad[:-num_validation_samples]\n",
    "y_train = sentiment[:-num_validation_samples]\n",
    "X_test_pad = review_pad[-num_validation_samples:]\n",
    "y_test = sentiment[-num_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_pad tensor: (40000, 100)\n",
      "Shape of y_train tensor: (40000,)\n",
      "Shape of X_test_pad tensor: (10000, 100)\n",
      "Shape of y_test tensor: (10000,)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of X_train_pad tensor:', X_train_pad.shape)\n",
    "print('Shape of y_train tensor:', y_train.shape)\n",
    "\n",
    "print('Shape of X_test_pad tensor:', X_test_pad.shape)\n",
    "print('Shape of y_test tensor:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM =100\n",
    "num_words = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i > num_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134157\n"
     ]
    }
   ],
   "source": [
    "print(num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 100)          13415700  \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 96, 128)           64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 48, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6144)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 6145      \n",
      "=================================================================\n",
      "Total params: 13,485,973\n",
      "Trainable params: 70,273\n",
      "Non-trainable params: 13,415,700\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/25\n",
      " - 16s - loss: 0.4121 - accuracy: 0.8113 - val_loss: 0.3536 - val_accuracy: 0.8442\n",
      "Epoch 2/25\n",
      " - 15s - loss: 0.2849 - accuracy: 0.8815 - val_loss: 0.3326 - val_accuracy: 0.8576\n",
      "Epoch 3/25\n",
      " - 15s - loss: 0.2287 - accuracy: 0.9097 - val_loss: 0.3710 - val_accuracy: 0.8472\n",
      "Epoch 4/25\n",
      " - 15s - loss: 0.1737 - accuracy: 0.9347 - val_loss: 0.3788 - val_accuracy: 0.8488\n",
      "Epoch 5/25\n",
      " - 15s - loss: 0.1174 - accuracy: 0.9621 - val_loss: 0.4166 - val_accuracy: 0.8444\n",
      "Epoch 6/25\n",
      " - 15s - loss: 0.0758 - accuracy: 0.9796 - val_loss: 0.4840 - val_accuracy: 0.8407\n",
      "Epoch 7/25\n",
      " - 15s - loss: 0.0444 - accuracy: 0.9922 - val_loss: 0.5203 - val_accuracy: 0.8441\n",
      "Epoch 8/25\n",
      " - 15s - loss: 0.0252 - accuracy: 0.9977 - val_loss: 0.5654 - val_accuracy: 0.8401\n",
      "Epoch 9/25\n",
      " - 15s - loss: 0.0144 - accuracy: 0.9996 - val_loss: 0.6012 - val_accuracy: 0.8413\n",
      "Epoch 10/25\n",
      " - 15s - loss: 0.0087 - accuracy: 0.9999 - val_loss: 0.6452 - val_accuracy: 0.8432\n",
      "Epoch 11/25\n",
      " - 15s - loss: 0.0057 - accuracy: 0.9999 - val_loss: 0.6718 - val_accuracy: 0.8431\n",
      "Epoch 12/25\n",
      " - 16s - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.7047 - val_accuracy: 0.8439\n",
      "Epoch 13/25\n",
      " - 16s - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.7273 - val_accuracy: 0.8426\n",
      "Epoch 14/25\n",
      " - 16s - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.7541 - val_accuracy: 0.8429\n",
      "Epoch 15/25\n",
      " - 16s - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.7789 - val_accuracy: 0.8439\n",
      "Epoch 16/25\n",
      " - 16s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.8014 - val_accuracy: 0.8437\n",
      "Epoch 17/25\n",
      " - 15s - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.8261 - val_accuracy: 0.8450\n",
      "Epoch 18/25\n",
      " - 15s - loss: 9.1584e-04 - accuracy: 1.0000 - val_loss: 0.8557 - val_accuracy: 0.8439\n",
      "Epoch 19/25\n",
      " - 15s - loss: 7.3788e-04 - accuracy: 1.0000 - val_loss: 0.8715 - val_accuracy: 0.8436\n",
      "Epoch 20/25\n",
      " - 16s - loss: 5.9658e-04 - accuracy: 1.0000 - val_loss: 0.8955 - val_accuracy: 0.8421\n",
      "Epoch 21/25\n",
      " - 15s - loss: 4.9307e-04 - accuracy: 1.0000 - val_loss: 0.9170 - val_accuracy: 0.8440\n",
      "Epoch 22/25\n",
      " - 15s - loss: 4.0323e-04 - accuracy: 1.0000 - val_loss: 0.9340 - val_accuracy: 0.8435\n",
      "Epoch 23/25\n",
      " - 15s - loss: 3.3077e-04 - accuracy: 1.0000 - val_loss: 0.9587 - val_accuracy: 0.8437\n",
      "Epoch 24/25\n",
      " - 16s - loss: 2.7301e-04 - accuracy: 1.0000 - val_loss: 0.9772 - val_accuracy: 0.8435\n",
      "Epoch 25/25\n",
      " - 15s - loss: 2.2797e-04 - accuracy: 1.0000 - val_loss: 0.9980 - val_accuracy: 0.8426\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f875b7a0310>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.initializers import Constant\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=max_length,\n",
    "                            trainable=False)\n",
    "\n",
    "model.add(embedding_layer)\n",
    "model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print(model.summary())\n",
    "\n",
    "# compile network\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# fit the model\n",
    "model.fit(X_train_pad, y_train, batch_size=128, epochs=25, validation_data=(X_test_pad, y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 138us/step\n",
      "Accuracy: 84.259999\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test_pad, y_test, batch_size=128)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99973947],\n",
       "       [0.6927332 ],\n",
       "       [0.67135173],\n",
       "       [0.43925798],\n",
       "       [0.7572388 ],\n",
       "       [0.00264442],\n",
       "       [0.6927332 ],\n",
       "       [0.02970614]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let us test some  samples\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "\n",
    "test_sample_1 = \"This movie is fantastic! I really like it because it is so good!\"\n",
    "test_sample_2 = \"Good movie!\"\n",
    "test_sample_3 = \"Maybe I like this movie.\"\n",
    "test_sample_4 = \"Not to my taste, will skip and watch another movie\"\n",
    "test_sample_5 = \"if you like action, then this movie might be good for you.\"\n",
    "test_sample_6 = \"Bad movie!\"\n",
    "test_sample_7 = \"Not a good movie!\"\n",
    "test_sample_8 = \"This movie really sucks! Can I get my money back please?\"\n",
    "test_samples = [test_sample_1, test_sample_2, test_sample_3, test_sample_4, test_sample_5, test_sample_6, test_sample_7, test_sample_8]\n",
    "\n",
    "test_samples_tokens = tokenizer_obj.texts_to_sequences(test_samples)\n",
    "test_samples_tokens_pad = pad_sequences(test_samples_tokens, maxlen=max_length)\n",
    "\n",
    "#predict\n",
    "model.predict(x=test_samples_tokens_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of the built model...\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 100, 100)          13415700  \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 32)                12768     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 13,428,501\n",
      "Trainable params: 12,801\n",
      "Non-trainable params: 13,415,700\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, GRU\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.initializers import Constant\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=max_length,\n",
    "                            trainable=False)\n",
    "model.add(embedding_layer)\n",
    "model.add(GRU(units=32,  dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print('Summary of the built model...')\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/25\n",
      " - 24s - loss: 247.7697 - accuracy: 0.5070 - val_loss: 0.7121 - val_accuracy: 0.5134\n",
      "Epoch 2/25\n",
      " - 24s - loss: 246.2586 - accuracy: 0.5077 - val_loss: 0.7086 - val_accuracy: 0.5224\n",
      "Epoch 3/25\n",
      " - 23s - loss: 1562.5152 - accuracy: 0.5167 - val_loss: 0.7093 - val_accuracy: 0.5187\n",
      "Epoch 4/25\n",
      " - 23s - loss: 105.1660 - accuracy: 0.5106 - val_loss: 0.7086 - val_accuracy: 0.5273\n",
      "Epoch 5/25\n",
      " - 23s - loss: 0.7189 - accuracy: 0.5145 - val_loss: 0.7074 - val_accuracy: 0.5284\n",
      "Epoch 6/25\n",
      " - 23s - loss: 1.3251 - accuracy: 0.5158 - val_loss: 0.7077 - val_accuracy: 0.5265\n",
      "Epoch 7/25\n",
      " - 24s - loss: 0.7377 - accuracy: 0.5142 - val_loss: 0.7077 - val_accuracy: 0.5283\n",
      "Epoch 8/25\n",
      " - 23s - loss: 0.7190 - accuracy: 0.5160 - val_loss: 0.7077 - val_accuracy: 0.5273\n",
      "Epoch 9/25\n",
      " - 23s - loss: 0.7176 - accuracy: 0.5183 - val_loss: 0.7076 - val_accuracy: 0.5286\n",
      "Epoch 10/25\n",
      " - 23s - loss: 0.8534 - accuracy: 0.5135 - val_loss: 0.7075 - val_accuracy: 0.5276\n",
      "Epoch 11/25\n",
      " - 23s - loss: 0.7187 - accuracy: 0.5164 - val_loss: 0.7074 - val_accuracy: 0.5280\n",
      "Epoch 12/25\n",
      " - 24s - loss: 0.7192 - accuracy: 0.5159 - val_loss: 0.7073 - val_accuracy: 0.5288\n",
      "Epoch 13/25\n",
      " - 23s - loss: 0.7188 - accuracy: 0.5165 - val_loss: 0.7071 - val_accuracy: 0.5283\n",
      "Epoch 14/25\n",
      " - 23s - loss: 0.7178 - accuracy: 0.5171 - val_loss: 0.7070 - val_accuracy: 0.5283\n",
      "Epoch 15/25\n",
      " - 24s - loss: 0.7334 - accuracy: 0.5160 - val_loss: 0.7070 - val_accuracy: 0.5293\n",
      "Epoch 16/25\n",
      " - 23s - loss: 0.7558 - accuracy: 0.5172 - val_loss: 0.7068 - val_accuracy: 0.5291\n",
      "Epoch 17/25\n",
      " - 24s - loss: 0.7177 - accuracy: 0.5168 - val_loss: 0.7065 - val_accuracy: 0.5293\n",
      "Epoch 18/25\n",
      " - 24s - loss: 0.7163 - accuracy: 0.5196 - val_loss: 0.7062 - val_accuracy: 0.5286\n",
      "Epoch 19/25\n",
      " - 23s - loss: 0.7182 - accuracy: 0.5150 - val_loss: 0.7059 - val_accuracy: 0.5297\n",
      "Epoch 20/25\n",
      " - 25s - loss: 0.7157 - accuracy: 0.5174 - val_loss: 0.7055 - val_accuracy: 0.5304\n",
      "Epoch 21/25\n",
      " - 24s - loss: 0.7176 - accuracy: 0.5168 - val_loss: 0.7050 - val_accuracy: 0.5305\n",
      "Epoch 22/25\n",
      " - 25s - loss: 0.7152 - accuracy: 0.5181 - val_loss: 0.7045 - val_accuracy: 0.5312\n",
      "Epoch 23/25\n",
      " - 23s - loss: 0.7155 - accuracy: 0.5193 - val_loss: 0.7039 - val_accuracy: 0.5320\n",
      "Epoch 24/25\n",
      " - 24s - loss: 0.7158 - accuracy: 0.5203 - val_loss: 0.7032 - val_accuracy: 0.5331\n",
      "Epoch 25/25\n",
      " - 29s - loss: 0.7141 - accuracy: 0.5198 - val_loss: 0.7024 - val_accuracy: 0.5351\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f875662aa90>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Train...')\n",
    "\n",
    "model.fit(X_train_pad, y_train, batch_size=128, epochs=25, validation_data=(X_test_pad, y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "10000/10000 [==============================] - 1s 147us/step\n",
      "Test score: 0.7024012398719788\n",
      "Test accuracy: 0.535099983215332\n",
      "Accuracy: 53.51%\n"
     ]
    }
   ],
   "source": [
    "print('Testing...')\n",
    "score, acc = model.evaluate(X_test_pad, y_test, batch_size=128)\n",
    "\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n",
    "\n",
    "print(\"Accuracy: {0:.2%}\".format(acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
